# ğŸŒªï¸ BertNado

**BertNado** is a Python package for predicting chromatin-binding protein (CBP) binding from DNA sequence using Hugging Face transformers.  
Built for **regression**, **binary**, and **multi-label classification** tasks with efficient fine-tuning via **PEFT/LoRA** and rich model interpretation.

---

## ğŸ§¬ Features

- âœ… Support for **regression**, **binary**, and **multi-label classification**
- ğŸ§ª Chromosome-aware train/val/test split
- âš¡ Efficient fine-tuning using **LoRA (Low-Rank Adaptation)**
- ğŸ¯ Hyperparameter optimization via **Bayesian W&B sweeps**
- ğŸ“Š Evaluation plots: **RÂ²**, **ROC**, **PR**, **confusion matrix**
- ğŸ§  Model interpretation via **SHAP** and **Captum Layer Integrated Gradients(LIG)**

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/CChahrour/bertnado.git
cd bertnado
pip install -e .
```

---

## ğŸ“ Project Structure

```
bertnado/
â”œâ”€â”€ cli.py                      # Command-line interface
â”œâ”€â”€ data/
â”‚   â””â”€â”€ prepare_dataset.py      # Dataset creation and tokenization
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ predict.py              # Predict from trained models
â”‚   â””â”€â”€ feature_extraction.py   # SHAP / LIG-based interpretation
â””â”€â”€ training/
    â”œâ”€â”€ finetune.py             # Fine-tuning using best config
    â”œâ”€â”€ full_train.py           # Full training loop
    â”œâ”€â”€ model.py                # PEFT/LoRA model architecture
    â”œâ”€â”€ sweep.py                # W&B sweep setup
    â”œâ”€â”€ trainers.py             # Trainer wrappers
    â””â”€â”€ metrics.py              # Metric computation
```

---

## ğŸš€ Quickstart

### Step 1: Prepare Dataset

```bash
bertnado-data \
  --file-path test/data/mock_data.parquet \
  --target-column test_A \
  --fasta-file test/data/mock_genome.fasta \
  --tokenizer-name PoetschLab/GROVER \
  --output-dir output/dataset \
  --task-type regression
```

---

### Step 2: Run Hyperparameter Sweep

```bash
bertnado-sweep \
  --config-path test/data/mock_sweep_config.json \
  --output-dir output/sweep \
  --model-name PoetschLab/GROVER \
  --dataset output/dataset \
  --sweep-count 2 \
  --project-name project \
  --task-type regression
```

---

### Step 3: Train Best Model

```bash
bertnado-train \
  --output-dir output/train \
  --model-name PoetschLab/GROVER \
  --dataset output/dataset \
  --best-config-path output/sweep/best_sweep_config.json \
  --task-type regression \
  --project-name project
```

---

### Step 4: Predict on Test Set

```bash
bertnado-predict \
  --tokenizer-name PoetschLab/GROVER \
  --model-dir output/train/model \
  --dataset-dir output/dataset \
  --output-dir output/predictions \
  --task-type regression
```

---

### Step 5: Interpret Model with SHAP or LIG

```bash
bertnado-feature \
  --tokenizer-name PoetschLab/GROVER \
  --model-dir output/train/model \
  --dataset-dir output/dataset \
  --output-dir output/feature_analysis \
  --task-type regression \
  --method shap
```

Run both SHAP and LIG:

```bash
--method both
```

---

## ğŸ“ˆ Outputs

- ğŸ“Š **Figures** saved to `output/figures/`
  - Regression: RÂ² scatter plot
  - Classification: ROC & PR curves
  - Binary: Confusion matrix

- ğŸ“‰ **SHAP scores** saved to `output/shap/`
- ğŸ“¦ **Trained models** saved to `output/models/`

---

## ğŸ§  Interpretation Tools

- **SHAP**: Global and local token importance
- **Captum LIG**: Gradient-based token attribution at the embedding level

---

## ğŸ§  Acknowledgements

- ğŸ¤— Hugging Face Transformers
- ğŸ§¬ PoetschLab/GROVER
- ğŸ“‰ PEFT/LoRA 
- ğŸ§  SHAP & Captum for interpretability
- ğŸ§¬ `crested` for efficient sequence extraction

---
