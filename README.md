# ğŸŒªï¸ BertNado

BertNado is a modular framework for fine-tuning Hugging Face DNA language models such as GROVER, NT2, and DNABERT variants on genomic prediction tasks. It supports both full fine-tuning and parameter-efficient transfer learning (PEFT) strategies like LoRA.

---

## ğŸ”§ Features

- âœ… Model Support: GROVER, NT2 (Nucleotide Transformer), DNABERT, and other Hugging Face-compatible DNA language models
- ğŸ§¬ Task Flexibility: Supports regression, binary, and multi-label classification, as well as masked DNA modeling
- ğŸ§ª Chromosome-aware Splits: Train/val/test split by chromosome to prevent data leakage
- âš¡ Efficient Fine-tuning: Drop-in support for parameter-efficient tuning methods like LoRA
- ğŸ¯ Hyperparameter Optimization: Integrated with Weights & Biases for Bayesian sweep-based tuning
- ğŸ“Š Robust Evaluation: Automatically generates RÂ², ROC, PR, and confusion matrix plots
- ğŸ§  Model Interpretation: SHAP and Captum Layer Integrated Gradients (LIG) for biological insight
- ğŸ§° Trainer Integration: Built on Hugging Face Trainer with custom heads and metrics
- ğŸ“ˆ W&B Logging: Full experiment tracking with Weights & Biases out of the box


---

## ğŸ“¦ Installation

```bash
git clone https://github.com/CChahrour/BertNado.git
cd BertNado
pip install -e .
```

---

## ğŸ“ Project Structure

```
bertnado/
â”œâ”€â”€ cli.py                      # Command-line interface
â”œâ”€â”€ data/
â”‚   â””â”€â”€ prepare_dataset.py      # Dataset creation and tokenization
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ predict.py              # Predict from trained models
â”‚   â””â”€â”€ feature_extraction.py   # SHAP / LIG-based interpretation
â””â”€â”€ training/
    â”œâ”€â”€ finetune.py             # Fine-tuning using best config
    â”œâ”€â”€ full_train.py           # Full training loop
    â”œâ”€â”€ model.py                # PEFT/LoRA model architecture
    â”œâ”€â”€ sweep.py                # W&B sweep setup
    â”œâ”€â”€ trainers.py             # Trainer wrappers
    â””â”€â”€ metrics.py              # Metric computation
```

---

## ğŸš€ Quickstart

### Step 1: Prepare Dataset

```bash
bertnado-data \
  --file-path test/data/mock_data.parquet \
  --target-column test_A \
  --fasta-file test/data/mock_genome.fasta \
  --tokenizer-name PoetschLab/GROVER \
  --output-dir output/dataset \
  --task-type regression
```

---

### Step 2: Run Hyperparameter Sweep

```bash
bertnado-sweep \
  --config-path test/data/mock_sweep_config.json \
  --output-dir output/sweep \
  --model-name PoetschLab/GROVER \
  --dataset output/dataset \
  --sweep-count 2 \
  --project-name project \
  --task-type regression
```

---

### Step 3: Train Best Model

```bash
bertnado-train \
  --output-dir output/train \
  --model-name PoetschLab/GROVER \
  --dataset output/dataset \
  --best-config-path output/sweep/best_sweep_config.json \
  --task-type regression \
  --project-name project
```

---

### Step 4: Predict on Test Set

```bash
bertnado-predict \
  --tokenizer-name PoetschLab/GROVER \
  --model-dir output/train/model \
  --dataset-dir output/dataset \
  --output-dir output/predictions \
  --task-type regression
```

---

### Step 5: Interpret Model with SHAP or LIG

```bash
bertnado-feature \
  --tokenizer-name PoetschLab/GROVER \
  --model-dir output/train/model \
  --dataset-dir output/dataset \
  --output-dir output/feature_analysis \
  --task-type regression \
  --method shap
```

Run both SHAP and LIG:

```bash
--method both
```

---

## ğŸ“ˆ Outputs

- ğŸ“Š **Figures** saved to `output/figures/`
  - Regression: RÂ² scatter plot
  - Classification: ROC & PR curves
  - Binary: Confusion matrix

- ğŸ“‰ **SHAP scores** saved to `output/shap/`
- ğŸ“¦ **Trained models** saved to `output/models/`

---

## ğŸ§  Interpretation Tools

- **SHAP**: Global and local token importance
- **Captum LIG**: Gradient-based token attribution at the embedding level

---

## ğŸ§  Acknowledgements

- ğŸ¤— Hugging Face Transformers
- ğŸ§¬ PoetschLab/GROVER
- ğŸ“‰ PEFT/LoRA 
- ğŸ§  SHAP & Captum for interpretability
- ğŸ§¬ `crested` for efficient sequence extraction

---
